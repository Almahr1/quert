crawler:
  max_pages: 10000
  max_depth: 5
  concurrent_workers: 10
  request_timeout: 30s
  user_agent: "LLMCrawler/1.0"

# Rate limiting
rate_limit:
  requests_per_second: 2.0
  burst: 10
  per_host_limit: true

# Content processing
content:
  min_text_length: 100
  max_text_length: 100000
  languages: ["en", "es", "fr", "de"]
  quality_threshold: 0.7

# Storage
storage:
  type: "file" # options: file, postgres, badger
  path: "./data"
  batch_size: 1000
  compression: true

# Monitoring
monitoring:
  metrics_port: 8080
  log_level: "info"
  enable_profiling: false