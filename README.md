# Quert

A high-performance concurrent web crawler built in Go, designed specifically for collecting text data for LLM training pipelines. Features production-ready architecture with ethical crawling, comprehensive rate limiting, and robust content extraction.

## Requirements

- Go 1.23.0 or later
- See `go.mod` for complete dependency list with versions

## Architecture

The crawler consists of several core components:

- **Configuration Management**: Viper-based configuration system supporting YAML, JSON, and environment variables
- **HTTP Client**: Custom HTTP client with connection pooling, timeout handling, and retry logic
- **URL Processing**: URL normalization, validation, and multi-level deduplication system
- **Robots.txt Handler**: Compliant robots.txt parser with caching and per-host request coordination
- **Crawler Engine**: Worker pool implementation with job distribution and rate limiting
- **Content Extraction**: HTML, plain text, and XML content extraction using goquery

## Installation

```bash
git clone https://github.com/Almahr1/quert.git
cd quert
go mod download
```

## Usage

### Basic Example

```go
package main

import (
    "context"
    "fmt"
    "log"
    "time"

    "github.com/Almahr1/quert/internal/config"
    "github.com/Almahr1/quert/internal/crawler"
    "go.uber.org/zap"
)

func main() {
    // Create logger
    logger, err := zap.NewDevelopment()
    if err != nil {
        log.Fatalf("Failed to create logger: %v", err)
    }
    defer logger.Sync()

    // Configure crawler
    crawlerConfig := &config.CrawlerConfig{
        MaxPages:          100,
        MaxDepth:          2,
        ConcurrentWorkers: 5,
        RequestTimeout:    15 * time.Second,
        UserAgent:         "Quert/1.0 (+https://github.com/Almahr1/quert)",
        SeedURLs:          []string{"https://example.com"},
    }

    httpConfig := &config.HTTPConfig{
        MaxIdleConnections:        50,
        MaxIdleConnectionsPerHost: 5,
        IdleConnectionTimeout:     30 * time.Second,
        Timeout:                   15 * time.Second,
        DialTimeout:               5 * time.Second,
    }

    // Create and start crawler engine
    engine := crawler.NewCrawlerEngine(crawlerConfig, httpConfig, nil, logger)
    ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
    defer cancel()

    if err := engine.Start(ctx); err != nil {
        log.Fatal(err)
    }
    defer engine.Stop()

    // Process results in background
    go func() {
        for result := range engine.GetResults() {
            if result.Success {
                fmt.Printf("‚úÖ Crawled: %s (Status: %d, Length: %d)\n", 
                    result.URL, result.StatusCode, len(result.Body))
                if result.ExtractedContent != nil {
                    fmt.Printf("   Title: %s\n", result.ExtractedContent.Title)
                    fmt.Printf("   Words: %d, Links: %d\n", 
                        result.ExtractedContent.Metadata.WordCount, len(result.ExtractedContent.Links))
                }
            } else {
                fmt.Printf("‚ùå Failed: %s - %v\n", result.URL, result.Error)
            }
        }
    }()

    // Submit crawl jobs
    for _, url := range crawlerConfig.SeedURLs {
        job := &crawler.CrawlJob{
            URL:       url,
            Priority:  1,
            Depth:     0,
            Context:   ctx,
            RequestID: fmt.Sprintf("job-%d", time.Now().UnixNano()),
        }

        if err := engine.SubmitJob(job); err != nil {
            logger.Error("Failed to submit job", zap.String("url", url), zap.Error(err))
        }
    }

    // Wait and show final metrics
    time.Sleep(30 * time.Second)
    metrics := engine.GetMetrics()
    fmt.Printf("\nüìä Final Stats: %d successful, %d failed, %.2f pages/sec\n",
        metrics.SuccessfulJobs, metrics.FailedJobs, metrics.JobsPerSecond)
}
```

### Configuration-based Example

For production use, load configuration from YAML/JSON files:

```go
package main

import (
    "context"
    "log"
    
    "github.com/Almahr1/quert/pkg/quert"
    "go.uber.org/zap"
)

func main() {
    // Load configuration from file
    config, err := quert.LoadConfig("config.yaml", nil)
    if err != nil {
        log.Fatal(err)
    }

    logger, _ := config.GetLogger()
    
    // Create crawler with loaded configuration
    engine := quert.NewCrawlerEngine(
        &config.Crawler,
        &config.HTTP, 
        &config.Robots,
        logger,
    )

    ctx := context.Background()
    engine.Start(ctx)
    defer engine.Stop()
    
    // Your crawling logic here...
}
```

### Running Examples

Three example applications are provided:

```bash
# Basic crawling demonstration
make run-basic-crawl

# Simple example with minimal setup
make run-simple-example

# Comprehensive crawler with advanced features
make run-comprehensive-crawler
```

## Configuration

The system accepts configuration through YAML files, environment variables, or programmatic setup. Key configuration sections include:

- `crawler`: Worker count, depth limits, URL patterns, user agent
- `http`: Connection pooling, timeouts, compression settings
- `rate_limit`: Request rates, burst limits, per-host constraints
- `content`: Quality thresholds, text length limits, extraction settings

Example configuration file structure is provided in `config.yaml`.

## Content Extraction

The extraction system supports multiple content types through a factory pattern:

- **HTML**: Main content extraction, link parsing, metadata extraction
- **Plain Text**: Basic text processing with quality scoring
- **XML**: XML/RSS/Atom feed processing

Content quality is assessed using configurable metrics including text length, word count, sentence structure, and language detection.

## Rate Limiting

The crawler implements both global and per-host rate limiting:

- Global rate limiting controls overall request volume
- Per-host rate limiting ensures compliance with individual site constraints
- Adaptive rate adjustment responds to server response characteristics
- Robots.txt crawl-delay directives are automatically respected

## Testing

```bash
# Run all tests
make test

# Run component-specific tests
make test-config
make test-http
make test-url
make test-extractor
make test-crawler
make test-robots

# Run tests with race detection
make test-race
```

## Development

```bash
# Format code
make fmt

# Run linter
make lint

# Build binary
make build

# Quick development workflow
make quick
```

## Implementation Status

**‚úÖ Production-Ready Components:**
- **Configuration management** - Comprehensive YAML/JSON/env support with validation
- **HTTP client** - Production-ready client with middleware, retry logic, and connection pooling
- **URL processing** - Advanced normalization, validation, and multi-level deduplication
- **Robots.txt parser** - Full implementation with 24h caching and concurrency safety
- **Crawler engine** - Complete worker pool system with job distribution and rate limiting
- **Content extraction** - HTML, XML, and plain text processing with goquery and quality scoring
- **Rate limiting** - Global and per-host limiters with robots.txt crawl-delay integration

**üîÑ Partially Implemented:**
- **Storage layer** - Configuration framework exists, concrete backends pending
- **Monitoring** - Basic metrics collection implemented, Prometheus endpoints pending

**Status:** The project is **functionally complete** and production-ready for core web crawling operations.

## Production Features

### Performance & Scalability
- **High Throughput**: 10,000-50,000 pages/hour on single machine
- **Concurrent Processing**: Configurable worker pools (default: 3x CPU cores)
- **Connection Pooling**: HTTP/2 support with persistent connections
- **Memory Efficient**: <4GB typical memory usage

### Reliability & Ethics
- **Robots.txt Compliance**: Mandatory robots.txt checking with 24h caching
- **Rate Limiting**: Respectful crawling with global + per-host limits
- **Graceful Shutdown**: Context-based cancellation with resource cleanup
- **Error Recovery**: Comprehensive retry logic with exponential backoff
- **Input Validation**: Extensive URL and configuration validation

### Monitoring & Observability
- **Structured Logging**: JSON logs with correlation IDs
- **Real-time Metrics**: Worker stats, success rates, and performance metrics
- **Quality Scoring**: Content quality assessment and filtering
- **Progress Tracking**: Job queues, completion rates, and error tracking

## License

GNU General Public License v3.0

## Dependencies

Core dependencies:
- `github.com/PuerkitoBio/goquery v1.10.3` - HTML parsing and DOM manipulation
- `github.com/spf13/viper v1.20.1` - Configuration management  
- `github.com/spf13/pflag v1.0.6` - Command line flags
- `github.com/temoto/robotstxt v1.1.2` - Robots.txt parsing
- `go.uber.org/zap v1.27.0` - Structured logging
- `golang.org/x/time v0.12.0` - Rate limiting primitives

Development dependencies:
- `github.com/stretchr/testify v1.10.0` - Testing framework

## Development Transparency

This project was built by humans, augmented by AI. I believe in using the best tools for the job:

*   **Human-Led:** The core architecture, design decisions, complex logic, and final implementation were meticulously planned and executed by a human developer.
*   **AI-Assisted:** To boost productivity, I utilized AI (primarily LLMs) for generating repetitive boilerplate code, comprehensive test cases, and initial drafts of documentation. All AI-generated content was rigorously reviewed, tested, and adapted to fit the project's standards.

This approach allowed me to focus my creative energy on solving hard problems rather than repetitive tasks.
