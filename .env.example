# Environment Variables for Go Web Crawler
# Copy this file to .env and modify the values as needed

# =============================================================================
# CRAWLER CONFIGURATION
# =============================================================================

# Basic crawler settings
CRAWLER_MAX_PAGES=10000
CRAWLER_MAX_DEPTH=5
CRAWLER_CONCURRENT_WORKERS=10
CRAWLER_REQUEST_TIMEOUT=30s
CRAWLER_USER_AGENT="LLMCrawler/1.0 (+https://github.com/Almahr1/quert)"

# Seed URLs (comma-separated)
CRAWLER_SEED_URLS="https://example.com,https://another-site.com"

# URL patterns to include/exclude (regex patterns, comma-separated)
CRAWLER_INCLUDE_PATTERNS=".*\\.html,.*\\.php"
CRAWLER_EXCLUDE_PATTERNS=".*\\.(jpg|jpeg|png|gif|pdf|doc|zip)"

# Domain restrictions
CRAWLER_ALLOWED_DOMAINS="example.com,subdomain.example.com"
CRAWLER_BLOCKED_DOMAINS="spam-site.com,low-quality.net"

# =============================================================================
# RATE LIMITING CONFIGURATION
# =============================================================================

# Global rate limiting
RATE_LIMIT_REQUESTS_PER_SECOND=2.0
RATE_LIMIT_BURST=10
RATE_LIMIT_PER_HOST_LIMIT=true

# Host-specific rate limiting
RATE_LIMIT_DEFAULT_DELAY=1s
RATE_LIMIT_ADAPTIVE=true
RATE_LIMIT_RESPECT_RETRY_AFTER=true

# Circuit breaker settings
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=30s
CIRCUIT_BREAKER_MAX_REQUESTS=3

# =============================================================================
# CONTENT PROCESSING CONFIGURATION
# =============================================================================

# Text extraction settings
CONTENT_MIN_TEXT_LENGTH=100
CONTENT_MAX_TEXT_LENGTH=100000
CONTENT_LANGUAGES="en,es,fr,de,it,pt,ru,zh,ja,ko"
CONTENT_QUALITY_THRESHOLD=0.7

# Content filtering
CONTENT_REMOVE_BOILERPLATE=true
CONTENT_EXTRACT_MAIN_CONTENT=true
CONTENT_PRESERVE_FORMATTING=false
CONTENT_NORMALIZE_WHITESPACE=true

# Deduplication settings
DEDUP_ENABLED=true
DEDUP_URL_FINGERPRINTING=true
DEDUP_CONTENT_FINGERPRINTING=true
DEDUP_SEMANTIC_SIMILARITY=true
DEDUP_SIMILARITY_THRESHOLD=0.85

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================

# Storage backend type: file, postgres, badger, s3
STORAGE_TYPE=file

# File storage settings
STORAGE_PATH=./data
STORAGE_BATCH_SIZE=1000
STORAGE_COMPRESSION=true
STORAGE_FILE_FORMAT=jsonl

# PostgreSQL settings
STORAGE_CONNECTION_STRING="postgresql://crawler:password@localhost:5432/crawler_db?sslmode=disable"
STORAGE_MAX_CONNECTIONS=25
STORAGE_MAX_IDLE_CONNECTIONS=5
STORAGE_CONNECTION_MAX_LIFETIME=300s

# BadgerDB settings
STORAGE_BADGER_PATH=./data/badger
STORAGE_BADGER_IN_MEMORY=false
STORAGE_BADGER_SYNC_WRITES=false

# S3/MinIO settings
STORAGE_S3_ENDPOINT=http://localhost:9000
STORAGE_S3_BUCKET=crawler-data
STORAGE_S3_REGION=us-east-1
STORAGE_S3_ACCESS_KEY=minioadmin
STORAGE_S3_SECRET_KEY=minioadmin123
STORAGE_S3_USE_SSL=false

# =============================================================================
# MONITORING AND OBSERVABILITY
# =============================================================================

# Logging configuration
MONITORING_LOG_LEVEL=info
MONITORING_LOG_FORMAT=json
MONITORING_LOG_FILE=./logs/crawler.log
MONITORING_LOG_ROTATION=true
MONITORING_LOG_MAX_SIZE=100MB
MONITORING_LOG_MAX_BACKUPS=10

# Metrics configuration
MONITORING_METRICS_ENABLED=true
MONITORING_METRICS_PORT=8080
MONITORING_METRICS_PATH=/metrics

# Health check configuration
MONITORING_HEALTH_PORT=8080
MONITORING_HEALTH_PATH=/health

# Profiling (development only)
MONITORING_ENABLE_PROFILING=false
MONITORING_PROFILING_PORT=6060

# Distributed tracing
TRACING_ENABLED=false
TRACING_JAEGER_ENDPOINT=http://localhost:14268/api/traces
TRACING_SERVICE_NAME=web-crawler
TRACING_SAMPLE_RATE=0.1

# =============================================================================
# REDIS CONFIGURATION (if using Redis for caching)
# =============================================================================

REDIS_ADDR=localhost:6379
REDIS_PASSWORD=""
REDIS_DB=0
REDIS_POOL_SIZE=10
REDIS_MIN_IDLE_CONNECTIONS=5
REDIS_MAX_RETRIES=3
REDIS_RETRY_DELAY=1s

# =============================================================================
# HTTP CLIENT CONFIGURATION
# =============================================================================

# Connection settings
HTTP_MAX_IDLE_CONNECTIONS=1000
HTTP_MAX_IDLE_CONNECTIONS_PER_HOST=100
HTTP_IDLE_CONNECTION_TIMEOUT=90s
HTTP_DISABLE_KEEP_ALIVES=false

# Request settings
HTTP_TIMEOUT=30s
HTTP_DIAL_TIMEOUT=5s
HTTP_TLS_HANDSHAKE_TIMEOUT=10s
HTTP_RESPONSE_HEADER_TIMEOUT=10s

# Compression and encoding
HTTP_DISABLE_COMPRESSION=false
HTTP_ACCEPT_ENCODING="gzip, deflate"

# =============================================================================
# FRONTIER CONFIGURATION
# =============================================================================

# URL frontier settings
FRONTIER_QUEUE_CAPACITY=100000
FRONTIER_PRIORITY_QUEUES=10
FRONTIER_POLITENESS_QUEUES=1000
FRONTIER_CHECKPOINT_INTERVAL=60s
FRONTIER_PERSISTENT_STATE=true

# URL processing
FRONTIER_URL_NORMALIZATION=true
FRONTIER_CANONICALIZATION=true
FRONTIER_REMOVE_FRAGMENTS=true
FRONTIER_SORT_QUERY_PARAMS=true

# =============================================================================
# ROBOTS.TXT CONFIGURATION
# =============================================================================

ROBOTS_ENABLED=true
ROBOTS_CACHE_DURATION=24h
ROBOTS_USER_AGENT=*
ROBOTS_CRAWL_DELAY_OVERRIDE=false
ROBOTS_RESPECT_CRAWL_DELAY=true

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# TLS settings
TLS_INSECURE_SKIP_VERIFY=false
TLS_MIN_VERSION=1.2

# Authentication (if needed)
AUTH_ENABLED=false
AUTH_TYPE=basic
AUTH_USERNAME=""
AUTH_PASSWORD=""
AUTH_TOKEN=""

# IP restrictions
SECURITY_ALLOWED_IPS=""
SECURITY_BLOCKED_IPS=""

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Development mode
DEV_MODE=false
DEV_MOCK_RESPONSES=false
DEV_DISABLE_RATE_LIMITING=false
DEV_VERBOSE_LOGGING=false

# Hot reload (for development with air)
DEV_HOT_RELOAD=true
DEV_HOT_RELOAD_PORT=8090

# =============================================================================
# DOCKER AND DEPLOYMENT SETTINGS
# =============================================================================

# Container settings
CONTAINER_PORT=8080
CONTAINER_HEALTH_CHECK_PATH=/health

# =============================================================================
# FEATURE FLAGS
# =============================================================================

# Enable/disable specific features
FEATURE_JAVASCRIPT_RENDERING=false
FEATURE_SEMANTIC_ANALYSIS=true
FEATURE_CONTENT_CLASSIFICATION=true
FEATURE_MULTI_LANGUAGE_PROCESSING=true
FEATURE_REAL_TIME_STREAMING=false

# =============================================================================
# EXTERNAL SERVICES
# =============================================================================

# Content extraction services
EXTRACTOR_SERVICE_URL=""
EXTRACTOR_API_KEY=""

# Language detection service
LANGUAGE_DETECTION_SERVICE=""
LANGUAGE_DETECTION_API_KEY=""

# Content quality scoring service
QUALITY_SCORING_SERVICE=""
QUALITY_SCORING_API_KEY=""

# =============================================================================
# BACKUP AND RECOVERY
# =============================================================================

# Backup settings
BACKUP_ENABLED=false
BACKUP_INTERVAL=24h
BACKUP_RETENTION_DAYS=30
BACKUP_STORAGE_PATH=./backups

# Recovery settings
RECOVERY_AUTO_RESUME=true
RECOVERY_CHECKPOINT_INTERVAL=300s